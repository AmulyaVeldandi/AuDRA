from __future__ import annotations

"""Use LLM reasoning to map findings to structured recommendations."""

import json
import math
import re
from dataclasses import dataclass
from typing import Dict, List, Literal, Optional

from src.guidelines.indexer import GuidelineChunk
from src.services.nim_llm import NemotronClient, NIMServiceError
from src.utils.logger import get_logger


@dataclass
class Recommendation:
    """Structured follow-up advice distilled from guideline chunks."""

    follow_up_type: str
    timeframe_months: Optional[int]
    urgency: Literal["routine", "semi-urgent", "urgent", "stat"]
    reasoning: str
    citation: str
    confidence: float


class RecommendationMatcher:
    """Select the best-matching guideline recommendation via LLM reasoning."""

    _OUTPUT_SCHEMA = {
        "type": "object",
        "properties": {
            "follow_up_type": {"type": "string", "minLength": 1},
            "timeframe_months": {"type": ["integer", "null"], "minimum": 0},
            "urgency": {
                "type": "string",
                "enum": ["routine", "semi-urgent", "urgent", "stat"],
            },
            "reasoning": {"type": "string", "minLength": 1},
            "citation": {"type": "string", "minLength": 1},
        },
        "required": ["follow_up_type", "urgency", "reasoning", "citation"],
        "additionalProperties": False,
    }

    def __init__(self, llm_client: NemotronClient) -> None:
        self._llm_client = llm_client
        self._logger = get_logger("audra.guidelines.matcher")

    # ------------------------------------------------------------------ #
    def match(
        self,
        finding: Dict[str, object],
        guidelines: List[GuidelineChunk],
        patient_context: Optional[Dict[str, object]] = None,
    ) -> Recommendation:
        """Return the most appropriate follow-up recommendation."""

        if not guidelines:
            raise ValueError("At least one guideline chunk is required for matching.")

        prompt = self._build_prompt(finding, guidelines, patient_context)
        self._logger.info(
            "Requesting recommendation from LLM.",
            extra={"context": {"finding": finding, "guideline_count": len(guidelines)}},
        )

        try:
            response = self._llm_client.generate_json(prompt, schema=self._OUTPUT_SCHEMA)
        except NIMServiceError as exc:
            self._logger.warning(
                "Nemotron call failed; falling back to heuristic recommendation.",
                extra={"context": {"error": str(exc)}},
            )
            return self._fallback_recommendation(guidelines)

        if not self.validate_recommendation(response, guidelines):
            self._logger.warning(
                "LLM response did not align with provided guidelines; applying fallback.",
                extra={"context": {"response": response}},
            )
            return self._fallback_recommendation(guidelines)

        timeframe = response.get("timeframe_months")
        if timeframe is not None:
            timeframe = int(timeframe)

        recommendation = Recommendation(
            follow_up_type=response["follow_up_type"],
            timeframe_months=timeframe,
            urgency=response["urgency"],
            reasoning=response["reasoning"],
            citation=response["citation"],
            confidence=0.85,
        )

        self._logger.info(
            "Recommendation generated.",
            extra={
                "context": {
                    "follow_up_type": recommendation.follow_up_type,
                    "timeframe_months": recommendation.timeframe_months,
                    "urgency": recommendation.urgency,
                    "citation": recommendation.citation,
                }
            },
        )
        return recommendation

    def validate_recommendation(
        self,
        rec: Dict[str, object],
        guidelines: List[GuidelineChunk],
    ) -> bool:
        """Ensure the LLM response is plausible and grounded."""

        citation = str(rec.get("citation", "")).lower()
        if not citation:
            return False

        known_sources = {chunk.source.lower() for chunk in guidelines}
        if not any(source in citation for source in known_sources):
            return False

        timeframe = rec.get("timeframe_months")
        if timeframe is not None:
            try:
                timeframe = int(timeframe)
            except (TypeError, ValueError):
                return False
            if timeframe < 0 or timeframe > 60:
                return False

        urgency = rec.get("urgency")
        if urgency not in {"routine", "semi-urgent", "urgent", "stat"}:
            return False

        follow_up_type = rec.get("follow_up_type")
        if not follow_up_type:
            return False

        return True

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #

    def _build_prompt(
        self,
        finding: Dict[str, object],
        guidelines: List[GuidelineChunk],
        patient_context: Optional[Dict[str, object]],
    ) -> str:
        finding_description = json.dumps(finding, ensure_ascii=False)
        patient_description = json.dumps(patient_context or {}, ensure_ascii=False)

        guideline_lines = []
        for index, chunk in enumerate(guidelines, start=1):
            snippet = self._truncate(chunk.recommendation or chunk.text, 700)
            guideline_lines.append(
                f"{index}. [{chunk.source}] {chunk.category}: {snippet}"
            )

        prompt = (
            "You are a clinical decision support system for radiology follow-up.\n\n"
            f"Finding: {finding_description}\n"
            f"Patient: {patient_description}\n\n"
            "Relevant Guidelines:\n"
            f"{chr(10).join(guideline_lines)}\n\n"
            "Based on the guidelines, determine:\n"
            "1. Is follow-up needed?\n"
            "2. If yes, what type (CT, MRI, biopsy, referral)?\n"
            "3. What timeframe (in months)?\n"
            "4. What urgency level (routine, urgent, stat)?\n"
            "5. Which guideline supports this recommendation?\n\n"
            "Return as JSON: "
            '{"follow_up_type": "...", "timeframe_months": 0, "urgency": "routine", '
            '"reasoning": "...", "citation": "..."}'
        )
        return prompt

    def _fallback_recommendation(self, guidelines: List[GuidelineChunk]) -> Recommendation:
        guideline = guidelines[0]
        timeframe = self._extract_timeframe_months(guideline.recommendation)

        follow_up_type = guideline.modality or self._infer_modality_from_text(guideline.recommendation)
        if not follow_up_type:
            follow_up_type = "Clinical follow-up"

        reasoning = (
            "Fallback recommendation derived directly from the most specific available guideline chunk. "
            f"Category: {guideline.category}."
        )

        return Recommendation(
            follow_up_type=follow_up_type,
            timeframe_months=timeframe,
            urgency="routine",
            reasoning=reasoning,
            citation=guideline.citation or guideline.source,
            confidence=0.5,
        )

    @staticmethod
    def _truncate(text: str, limit: int) -> str:
        if len(text) <= limit:
            return text
        return text[: limit - 3].rstrip() + "..."

    @staticmethod
    def _extract_timeframe_months(text: str) -> Optional[int]:
        if not text:
            return None

        matches = re.findall(r"(\d+)\s*(?:-|to)?\s*(\d+)?\s*(?:month|mo)", text, flags=re.IGNORECASE)
        timeframes: List[int] = []
        for start, end in matches:
            start_val = int(start)
            if end:
                end_val = int(end)
                timeframes.append(min(start_val, end_val))
            else:
                timeframes.append(start_val)

        if not timeframes:
            numeric_matches = re.findall(r"\b(\d{1,2})\s*(?:wk|week)\b", text, flags=re.IGNORECASE)
            for weeks in numeric_matches:
                try:
                    months = max(1, math.ceil(int(weeks) / 4.0))
                    timeframes.append(months)
                except ValueError:
                    continue

        return min(timeframes) if timeframes else None

    @staticmethod
    def _infer_modality_from_text(text: str) -> Optional[str]:
        lowered = text.lower()
        if "pet" in lowered and "ct" in lowered:
            return "PET-CT"
        if "biopsy" in lowered:
            return "Biopsy"
        if "mri" in lowered:
            return "MRI"
        if "ultrasound" in lowered or "sonograph" in lowered:
            return "Ultrasound"
        if "ct" in lowered:
            return "CT"
        return None

